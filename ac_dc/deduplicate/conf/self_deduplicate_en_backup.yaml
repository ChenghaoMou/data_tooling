tokenization: "character"   # character, punctuation or space
window_size: 4              # size of the token window
hamming_distance: 3         # similarity threshold out of 64 bits
num_blocks: 5               # must be larger than the hamming_distance
ignore_punctuation: true    # ignore punctuation when hashing, cannot be true when punctuation is used for tokenization
lowercase: true             # lowercase the text when hashing
text_column: "text"         # column name for the text to be hashed
index_column: "id"          # column name for the index
num_proc: 80                # number of processes to run when hashing
load_dataset:
  path: null
  name: null
  split: null
  use_auth_token: false
load_from_disk:
  path: "/home/chenghao/data_tooling/ac_dc/deduplicate/cache/filtered_tot"
  gcs: null
cache: "outputs/en_cache"
output: "outputs/en"
